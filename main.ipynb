{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T13:27:31.986812Z",
     "start_time": "2024-10-24T13:27:25.917635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports des bibliothèques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports des données\n",
    "df_train = pd.read_csv(\"./data/train.csv\")\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# Valeurs manquantes\n",
    "df_train['Age'].fillna(df_train['Age'].median(), inplace=True)\n",
    "df_test['Age'].fillna(df_train['Age'].median(), inplace=True)\n",
    "df_train['Embarked'].fillna(df_train['Embarked'].mode()[0], inplace=True)\n",
    "df_test['Embarked'].fillna(df_train['Embarked'].mode()[0], inplace=True)\n",
    "df_test['Fare'].fillna(df_train['Fare'].median(), inplace=True)\n",
    "\n",
    "# Extraction des titres\n",
    "df_train['Title'] = df_train['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "df_test['Title'] = df_test['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# Regrouper les titres\n",
    "common_titles = ['Mr', 'Miss', 'Mrs', 'Master']\n",
    "df_train['Title'] = df_train['Title'].apply(lambda x: x if x in common_titles else 'Rare')\n",
    "df_test['Title'] = df_test['Title'].apply(lambda x: x if x in common_titles else 'Rare')\n",
    "\n",
    "# Encoder les colonnes catégorielles y compris 'Title'\n",
    "categorical_columns = ['Sex', 'Embarked', 'Title']\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    df_train[col] = label_encoders[col].fit_transform(df_train[col])\n",
    "    df_test[col] = label_encoders[col].transform(df_test[col])\n",
    "\n",
    "# On supprime les colonnes inutiles\n",
    "columns_to_drop = ['Cabin', 'Ticket', 'Name']\n",
    "df_train.drop(columns_to_drop, axis=1, inplace=True)\n",
    "df_test.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Inclure le titre dans les caractéristiques\n",
    "X = df_train.drop('Survived', axis=1)\n",
    "# 'Title' est déjà inclus dans X car il fait partie de df_train après encodage\n",
    "\n",
    "# Appliquer la même transformation au test set\n",
    "df_test_features = df_test.copy()\n",
    "\n",
    "# Séparation des features et de la target\n",
    "Y = df_train['Survived']\n",
    "\n",
    "# On sépare les données en train et validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Dimensionnement\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "df_test_scaled = scaler.transform(df_test_features)\n",
    "\n",
    "# Définition de la grille des hyperparamètres\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Entraînement du modèle avec GridSearchCV\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=42)\n",
    "grid_search_gb = GridSearchCV(\n",
    "    estimator=gradient_boosting,\n",
    "    param_grid=param_grid_gb,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# On entraîne le modèle\n",
    "grid_search_gb.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# On récupère le meilleur modèle\n",
    "best_gradient_boosting = grid_search_gb.best_estimator_\n",
    "\n",
    "# On affiche les meilleurs paramètres\n",
    "print(f\"Meilleurs paramètres: {grid_search_gb.best_params_}\")\n",
    "\n",
    "# Évaluation sur le jeu de validation\n",
    "Y_val_pred = best_gradient_boosting.predict(X_val_scaled)\n",
    "f1_score_val = metrics.f1_score(Y_val, Y_val_pred, average='macro')\n",
    "print(f'Validation Macro F1 Score: {f1_score_val}')\n",
    "\n",
    "# Prédiction sur le test set\n",
    "test_predictions = best_gradient_boosting.predict(df_test_scaled)\n",
    "\n",
    "# Créer un DataFrame avec les prédictions\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': df_test['PassengerId'],\n",
    "    'Survived': test_predictions\n",
    "})\n",
    "\n",
    "# Exporter en CSV\n",
    "submission.to_csv('./data/submission.csv', index=False)\n",
    "print(\"Fichier de soumission créé avec succès !\")\n"
   ],
   "id": "7054c82c937127ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Meilleurs paramètres: {'learning_rate': 0.2, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Validation Macro F1 Score: 0.8085714285714285\n",
      "Fichier de soumission créé avec succès !\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d4e7e7578aa3e89b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
