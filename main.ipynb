{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T13:23:26.225011Z",
     "start_time": "2024-10-23T13:23:19.467382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports des bibliothèques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports des données\n",
    "df_train = pd.read_csv(\"./data/train.csv\")\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# Valeurs manquantes\n",
    "df_train['Age'].fillna(df_train['Age'].median(), inplace=True)\n",
    "df_test['Age'].fillna(df_train['Age'].median(), inplace=True)\n",
    "df_train['Embarked'].fillna(df_train['Embarked'].mode()[0], inplace=True)\n",
    "df_test['Embarked'].fillna(df_train['Embarked'].mode()[0], inplace=True)\n",
    "df_test['Fare'].fillna(df_train['Fare'].median(), inplace=True)\n",
    "\n",
    "# On supprime les colonnes inutiles\n",
    "columns_to_drop = ['Cabin', 'Ticket', 'Name']\n",
    "df_train.drop(columns_to_drop, axis=1, inplace=True)\n",
    "df_test.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# On met des colonnes catégorielles en string\n",
    "categorical_columns = ['Sex', 'Embarked']\n",
    "for col in categorical_columns:\n",
    "    df_train[col] = df_train[col].astype(str)\n",
    "    df_test[col] = df_test[col].astype(str)\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    df_train[col] = label_encoders[col].fit_transform(df_train[col])\n",
    "    df_test[col] = label_encoders[col].transform(df_test[col])\n",
    "\n",
    "# Séparation des features et de la target\n",
    "X = df_train.drop('Survived', axis=1)\n",
    "Y = df_train['Survived']\n",
    "\n",
    "# On sépare les données en train et validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dimensionnement\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "df_test_scaled = scaler.transform(df_test)\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Entraînement du model avec GridSearchCV\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=42)\n",
    "grid_search_gb = GridSearchCV(\n",
    "    estimator=gradient_boosting,\n",
    "    param_grid=param_grid_gb,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# On entraîne le modèle\n",
    "grid_search_gb.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# On récupère le meilleur modèle\n",
    "best_gradient_boosting = grid_search_gb.best_estimator_\n",
    "\n",
    "# On affiche les meilleurs paramètres\n",
    "Y_test_pred = best_gradient_boosting.predict(X_test_scaled)\n",
    "f1_score_test = metrics.f1_score(Y_test, Y_test_pred, average='macro')\n",
    "print(f'Validation Macro F1 Score: {f1_score_test}')\n",
    "\n",
    "# Prédiction sur le test set\n",
    "test_predictions = best_gradient_boosting.predict(df_test_scaled)\n",
    "\n",
    "# Créer un DataFrame avec les prédictions\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': df_test['PassengerId'],\n",
    "    'Survived': test_predictions\n",
    "})\n",
    "\n",
    "# Exporter en CSV\n",
    "submission.to_csv('./data/submission.csv', index=False)\n",
    "print(\"Fichier de soumission créé avec succès !\")\n",
    "\n",
    "# Vérifier les premières lignes du fichier\n",
    "print(\"\\nAperçu des premières lignes :\")\n",
    "print(submission.head())"
   ],
   "id": "7054c82c937127ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Validation Macro F1 Score: 0.8123197903014416\n",
      "Fichier de soumission créé avec succès !\n",
      "\n",
      "Aperçu des premières lignes :\n",
      "   PassengerId  Survived\n",
      "0          892         0\n",
      "1          893         1\n",
      "2          894         0\n",
      "3          895         0\n",
      "4          896         0\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d4e7e7578aa3e89b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
